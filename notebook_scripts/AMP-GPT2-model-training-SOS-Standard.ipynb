{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656682a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3276f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose train mode\n",
    "global mode\n",
    "# mode = 'pretrain'\n",
    "mode = 'finetune'\n",
    "# mode = 'else'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d313ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a219b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='', vocab_size=23, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '[PAD]'})\n"
     ]
    }
   ],
   "source": [
    "# device and tokenizer\n",
    "\n",
    "# device = torch.device('cuda:0,1')\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer('./vocab_file/vocab.json', './vocab_file/merges.txt')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "if mode == 'pretrain':\n",
    "    tokenizer.save_pretrained('../save_model/benchmarks_models/train_raw_APD3_500epochs_model/') # when pretrain model\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9eff80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        if mode == 'finetune':\n",
    "#             with open('../data/benchmarks_data/bigclustr09.txt') as f:  # when finetune\n",
    "            with open('../data/benchmarks_data/nature_AMPs10-64.txt') as f:  # when finetune\n",
    "                lines = f.readlines()\n",
    "        elif mode == 'pretrain':\n",
    "            with open('../data/ADP3_amp.txt') as f:\n",
    "#             with open('./data/pretrain_data/uniprot10-63.txt') as f:  # when pretrain model \n",
    "                lines = f.readlines()\n",
    "        else:\n",
    "            print('train mode error')\n",
    "            with open('./data/ADP3_amp.txt') as f:\n",
    "                lines = f.readlines()\n",
    "        lines = [i.strip() for i in lines]\n",
    "\n",
    "        self.lines = lines\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.lines[i]\n",
    "\n",
    "\n",
    "global val_split\n",
    "if mode == 'pretrian':\n",
    "    val_split = 0.1\n",
    "elif mode =='finetune':\n",
    "    val_split = 0.1\n",
    "else:\n",
    "    val_split = 0.1\n",
    "\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(val_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = Data.SubsetRandomSampler(train_indices)\n",
    "val_sampler = Data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "def collate_fn(data):\n",
    "    data = tokenizer.batch_encode_plus(data,\n",
    "                                       padding=True,\n",
    "                                       truncation=True,\n",
    "                                       max_length=64,\n",
    "                                       return_tensors='pt')\n",
    "\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64, \n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,)\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64, \n",
    "    sampler=val_sampler,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,)\n",
    "\n",
    "# for i, data in enumerate(val_loader):\n",
    "    \n",
    "\n",
    "#     for k, v in data.items():\n",
    "#         print(k, v.shape, v)\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de835ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define GPT model\n",
    "\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config(n_layer=12, \n",
    "                           n_head=12,\n",
    "                           n_embd=768)\n",
    "\n",
    "print(configuration)\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "if mode == 'pretrain':\n",
    "    model = GPT2LMHeadModel(configuration)  # when pretrain model\n",
    "elif mode == 'finetune':\n",
    "    model = GPT2LMHeadModel.from_pretrained('../save_model/pretrain_model/pretrained-GPT-10-64washed-30epochs/')\n",
    "else:\n",
    "    pass\n",
    "#     model = GPT2LMHeadModel.from_pretrained('./save_model/pretrain_model/pretrained-GPT-10-64raw-20epochs/')\n",
    "# model = torch.load('./save_model/pretrained-gpt-10-48-30epochs/pytorch_model.bin')  # pretrain model use\n",
    "# model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f560aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "from torchmetrics import Accuracy \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device_ids = [0, 1]  # 使用第一和第二个GPU\n",
    "else:\n",
    "    device_ids = None\n",
    "    \n",
    "accuracy = Accuracy(task=\"multiclass\",num_classes=2300,ignore_index=23)\n",
    "accuracy = accuracy.to(device) \n",
    "\n",
    "epochs = 125\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list =[]\n",
    "val_acc_list = []\n",
    "    \n",
    "#训练\n",
    "def train():\n",
    "    global model\n",
    "\n",
    "    model = model.to(device)\n",
    "#     model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-7)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=400,\n",
    "                              num_training_steps=len(train_loader)*epochs,\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    model.train()\n",
    "    print('开始训练')\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        val_loss = []\n",
    "        val_accuracy = []\n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            out = model(**batch_data)\n",
    "            loss = out['loss']\n",
    "    \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            labels = batch_data['labels'][:, 1:]\n",
    "            out = out['logits'].argmax(dim=2)[:, :-1]\n",
    "            train_acc = accuracy(out, labels)\n",
    "            train_accuracy.append(train_acc.tolist())\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print('train_batch: {:3d}  loss:{:.4f}  accuracy:{:.4f}'\n",
    "                      .format(batch_idx, loss.item(), train_acc.item()))\n",
    "            \n",
    "        for batch_idx, batch_data in enumerate(val_loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            out = model(**batch_data)\n",
    "            loss = out['loss']\n",
    "            labels = batch_data['labels'][:, 1:]\n",
    "            out = out['logits'].argmax(dim=2)[:, :-1]\n",
    "            val_acc = accuracy(out, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            val_accuracy.append(val_acc.tolist())\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print('val_batch:   {:3d}  loss:{:.4f}  accuracy:{:.4f}'\n",
    "                      .format(batch_idx, loss.item(), val_acc.item()))\n",
    "            \n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        train_acc_list.append(np.mean(train_accuracy))\n",
    "        val_loss_list.append(np.mean(val_loss))\n",
    "        val_acc_list.append(np.mean(val_accuracy))\n",
    "        \n",
    "        train_time = time.time()\n",
    "        print('第{}代训练完成,历时{}秒'.format(epoch+1,train_time-start_time))\n",
    "        print('epoch {} mean training loss:{:.4f}'.format(epoch+1, np.mean(train_loss)))\n",
    "        print('epoch {} mean training accuracy:{:.4f}'.format(epoch+1, np.mean(train_accuracy)))\n",
    "        print('epoch {} mean val loss:{:.4f}'.format(epoch+1, np.mean(val_loss)))\n",
    "        print('epoch {} mean val accuracy:{:.4f} '.format(epoch+1, np.mean(val_accuracy)))\n",
    "        print(' ')\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('训练结束,训练时长：',end_time-start_time, '秒')   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cb1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n",
      "train_batch:   0  loss:1.5129  accuracy:0.1348\n",
      "train_batch:  50  loss:1.5532  accuracy:0.1328\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aead84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "if mode == 'pretrain':\n",
    "    model.save_pretrained('../save_model/benchmarks_models/train_raw_APD3_125epochs_model/')  # when pretrain model\n",
    "elif mode == 'finetune':\n",
    "    torch.save(model, '../save_model/benchmarks_models/nature_AMPs-125epochs_model')  # finetune model\n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_acc(save_dir, mode, epochs):\n",
    "    plt.figure(figsize=(12, 12),dpi=300,)\n",
    "\n",
    "    # Training and Validation Loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(1, len(train_loss_list) + 1), train_loss_list, label='Training Loss', linestyle='-', marker='o')\n",
    "    plt.plot(range(1, len(val_loss_list) + 1), val_loss_list, label='Validation Loss', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.title('Training and Validation Loss', fontsize=16)\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Training and Validation Accuracy\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(1, len(train_acc_list) + 1), train_acc_list, label='Training Accuracy', linestyle='-', marker='o')\n",
    "    plt.plot(range(1, len(val_acc_list) + 1), val_acc_list, label='Validation Accuracy', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.title('Training and Validation Accuracy', fontsize=16)\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}{mode}{epochs}_Training_and_Validation_Loss_and_Accuracy.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (note: 'mode' and 'epochs' should be defined)\n",
    "save_dir = '/home/xms/AMP-master/save_model/benchmarks_models/training_pdf/'\n",
    "plot_loss_acc(save_dir, 'nature_AMPs-', '125')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c87246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# def plot_loss_acc(save_dir, mode, epochs):\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "\n",
    "#     plt.subplot(2, 2, 1)\n",
    "#     plt.plot(range(1, len(train_loss_list) + 1), train_loss_list, label='train_loss')\n",
    "#     plt.plot(range(1, len(val_loss_list) + 1), val_loss_list, label='val_loss')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Training and Validation Loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     plt.plot(range(1, len(train_acc_list) + 1), train_acc_list, label='train_acc')\n",
    "#     plt.plot(range(1, len(val_acc_list) + 1), val_acc_list, label='val_acc')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.title('Training and Validation Accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_dir+mode+str(epochs)+'_Training and Validation Loss and Accuracy')  # 保存损失和准确率曲线图\n",
    "#     plt.show()\n",
    "    \n",
    "# save_dir = '/home/xms/AMP-master/generate_file/train_acc_loss/'\n",
    "# plot_loss_acc(save_dir, mode, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2644c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 结果作图\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 = [(x+1) for x in range(len(train_loss_list))]\n",
    "# x2 = [(x+1) for x in range(len(val_acc_list))]\n",
    "# y1 = train_loss_list\n",
    "# y2 = train_acc_list\n",
    "# y3 = val_loss_list\n",
    "# y4 = val_acc_list\n",
    "\n",
    "# plt.plot(x1, y1, label=\"AMP training loss\")\n",
    "# plt.plot(x1, y3, label=\"AMP val_loss\")\n",
    "# plt.xlabel('step')\n",
    "# plt.ylabel('loss')\n",
    "# plt.title('AMP train losses show')\n",
    "# plt.legend()\n",
    "# if mode == 'pretrain':\n",
    "#     plt.savefig('/xms/AMP-master/generate_file/train_graphical_result-2023/pretrain_result/'+\n",
    "#                 mode+'10-48washed-'+str(epochs)+'epochs_loss.jpg')\n",
    "# elif mode == 'finetune':\n",
    "#     plt.savefig('/home/xms/AMP-master/generate_file/train_acc_loss/'+\n",
    "#                 mode+'10-64washed-'+str(epochs)+'epochs_loss.jpg')\n",
    "# else:\n",
    "#     pass\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(x2, y2, label=\"AMP train_acc curse\")\n",
    "# plt.plot(x2, y4, label=\"AMP val_acc curse\")\n",
    "# plt.xlabel('step')\n",
    "# plt.ylabel('acc')\n",
    "# plt.title('AMP val_acc show')\n",
    "# plt.legend()\n",
    "# if mode == 'pretrain':\n",
    "#     plt.savefig('/xms/AMP-master/generate_file/train_graphical_result-2023/pretrain_result/'+\n",
    "#                 mode+'10-48washed-'+str(epochs)+'epochs_accuracy.jpg')\n",
    "# elif mode == 'finetune':\n",
    "#     plt.savefig('/home/xms/AMP-master/generate_file/train_acc_loss/'+\n",
    "#                 mode+'10-64washed-'+str(epochs)+'epochs_accuracy.jpg')\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b3841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
